# Sistema Inteligente de Matching Vaga-Candidato

## ğŸ“„ DescriÃ§Ã£o do Projeto

Este projeto apresenta um sistema avanÃ§ado de Machine Learning projetado para otimizar o processo de recrutamento e seleÃ§Ã£o de talentos. O objetivo principal Ã© automatizar e aprimorar a compatibilidade entre candidatos e vagas disponÃ­veis, utilizando algoritmos de aprendizado de mÃ¡quina para identificar os "melhores matches" com alta precisÃ£o. Em um cenÃ¡rio onde o volume de candidaturas pode ser esmagador, esta ferramenta visa reduzir significativamente o tempo gasto por recrutadores na triagem manual, permitindo que se concentrem em interaÃ§Ãµes mais estratÃ©gicas com candidatos qualificados.

O sistema processa dados de currÃ­culos de candidatos e descriÃ§Ãµes de vagas, transformando informaÃ§Ãµes textuais e estruturadas em features numÃ©ricas que podem ser compreendidas por modelos de Machine Learning. A arquitetura do projeto Ã© modular, garantindo escalabilidade, manutenibilidade e reprodutibilidade de todo o pipeline de dados e modelagem. Desde o prÃ©-processamento de dados brutos (JSONs) atÃ© a engenharia de features complexas, treinamento de modelos preditivos e avaliaÃ§Ã£o de performance, cada etapa Ã© cuidadosamente orquestrada para entregar resultados robustos e acionÃ¡veis.

O resultado final Ã© uma aplicaÃ§Ã£o interativa construÃ­da com Streamlit, que oferece uma interface amigÃ¡vel para recrutadores. AtravÃ©s dela, Ã© possÃ­vel selecionar uma vaga especÃ­fica e visualizar uma lista ranqueada de candidatos com base em seu score de compatibilidade, permitindo uma tomada de decisÃ£o mais rÃ¡pida e informada. Este sistema nÃ£o apenas acelera o processo de matching, mas tambÃ©m busca introduzir uma camada de objetividade e consistÃªncia na avaliaÃ§Ã£o de candidatos, complementando a expertise humana com o poder da inteligÃªncia artificial.

## ğŸ“„ Sobre o modelo utilizado

Queremos apoiar empregadores e candidatos na previsÃ£o da compatibilidade entre vagas.

ExperiÃªncia profissional do candidato
- Habilidades tÃ©cnicas listadas no currÃ­culo
- LocalizaÃ§Ã£o geogrÃ¡fica
- NÃ­vel profissional (JÃºnior, Pleno, SÃªnior)
- HistÃ³rico de sucesso de recrutadores e cidades (via Target Encoding)
- MÃ©tricas de Performance

O modelo foi avaliado e apresenta as seguintes mÃ©tricas:
- AUC: ~0.87 (Excelente capacidade de discriminaÃ§Ã£o)
- F1-Score: ~0.68 (Boa precisÃ£o e recall balanceados)
- AcurÃ¡cia: ~78% (Alta taxa de acertos)

Como Interpretar os Scores
- Score > 0.8: Candidato altamente compatÃ­vel
- Score 0.6 - 0.8: Candidato com boa compatibilidade
- Score 0.4 - 0.6: Candidato com compatibilidade moderada
- Score < 0.4: Candidato com baixa compatibilidade

LimitaÃ§Ãµes
O modelo Ã© baseado em dados histÃ³ricos e pode ter vieses
NÃ£o considera aspectos subjetivos como fit cultural
Recomenda-se sempre uma anÃ¡lise manual final do recrutador

## ğŸ› ï¸ Stack Utilizada

Este projeto foi desenvolvido utilizando uma combinaÃ§Ã£o de tecnologias e bibliotecas Python, focando em robustez, eficiÃªncia e facilidade de uso para o desenvolvimento de soluÃ§Ãµes de Machine Learning e aplicaÃ§Ãµes web interativas. A escolha da stack reflete as melhores prÃ¡ticas em MLOps (Machine Learning Operations), garantindo um pipeline de dados e modelo bem estruturado e escalÃ¡vel.

### Linguagem de ProgramaÃ§Ã£o

*   **Python**: A linguagem principal utilizada em todo o projeto, desde o prÃ©-processamento de dados atÃ© o desenvolvimento do modelo e da interface web. Sua vasta gama de bibliotecas e a comunidade ativa a tornam ideal para projetos de Machine Learning.

### Bibliotecas Principais

*   **Pandas**: Essencial para manipulaÃ§Ã£o e anÃ¡lise de dados, utilizada extensivamente nas etapas de prÃ©-processamento e engenharia de features. Facilita a leitura, limpeza, transformaÃ§Ã£o e agregaÃ§Ã£o de grandes volumes de dados.
*   **NumPy**: Fundamental para operaÃ§Ãµes numÃ©ricas de alta performance, servindo como base para muitas outras bibliotecas cientÃ­ficas em Python.
*   **Scikit-learn**: Uma biblioteca abrangente para Machine Learning, utilizada para tarefas como divisÃ£o de dados (treino/teste), prÃ©-processamento (e.g., `MinMaxScaler`, `LabelEncoder`), e avaliaÃ§Ã£o de modelos (e.g., `roc_auc_score`, `f1_score`, `accuracy_score`, `classification_report`, `confusion_matrix`).
*   **LightGBM**: O algoritmo de Machine Learning escolhido para o treinamento do modelo preditivo. Conhecido por sua alta performance, velocidade e eficiÃªncia no manuseio de grandes datasets, Ã© uma excelente escolha para problemas de classificaÃ§Ã£o como o matching de vagas.
*   **Optuna**: Utilizado para a otimizaÃ§Ã£o de hiperparÃ¢metros do modelo LightGBM. Optuna Ã© uma biblioteca de otimizaÃ§Ã£o automÃ¡tica de hiperparÃ¢metros que permite encontrar as melhores configuraÃ§Ãµes para o modelo de forma eficiente, melhorando significativamente a performance.
*   **Streamlit**: Framework utilizado para construir a interface de usuÃ¡rio interativa do sistema de matching. Permite transformar scripts Python em aplicaÃ§Ãµes web ricas e dinÃ¢micas com poucas linhas de cÃ³digo, ideal para prototipagem e demonstraÃ§Ãµes rÃ¡pidas.
*   **Boto3**: A SDK (Software Development Kit) da AWS para Python, utilizada para interagir com serviÃ§os da Amazon Web Services, especificamente para carregar e salvar arquivos CSV no Amazon S3, garantindo o armazenamento escalÃ¡vel e seguro dos dados processados.
*   **python-dotenv**: Utilizada para carregar variÃ¡veis de ambiente de um arquivo `.env`, facilitando o gerenciamento de credenciais e configuraÃ§Ãµes sensÃ­veis (como chaves de acesso da AWS) sem expÃ´-las diretamente no cÃ³digo-fonte.
*   **Joblib**: Biblioteca para serializaÃ§Ã£o e desserializaÃ§Ã£o de objetos Python, utilizada para salvar e carregar o modelo treinado (`model.pkl`) e as colunas do modelo (`model_columns.pkl`), permitindo que o modelo seja persistido e reutilizado sem a necessidade de retreinamento.
*   **Matplotlib** e **Seaborn**: Bibliotecas para criaÃ§Ã£o de grÃ¡ficos e visualizaÃ§Ãµes de dados. Embora a versÃ£o final do Streamlit possa ter optado por grÃ¡ficos mais simples do prÃ³prio Streamlit, estas bibliotecas sÃ£o cruciais para a AnÃ¡lise ExploratÃ³ria de Dados (EDA) e a visualizaÃ§Ã£o de mÃ©tricas de avaliaÃ§Ã£o do modelo em notebooks de desenvolvimento.

### Gerenciamento de DependÃªncias

*   **`requirements.txt`**: Um arquivo que lista todas as dependÃªncias do projeto, permitindo que o ambiente de desenvolvimento seja facilmente replicado em qualquer mÃ¡quina.

### Estrutura de Projeto

O projeto segue uma estrutura modular, com pastas dedicadas para cÃ³digo-fonte (`src`), dados (`data`), modelos (`models`), notebooks (`notebooks`), e a aplicaÃ§Ã£o Streamlit (`app`), promovendo a organizaÃ§Ã£o e a manutenibilidade do cÃ³digo.



## ğŸš€ Como Rodar o Aplicativo Localmente

Para executar a aplicaÃ§Ã£o Streamlit em sua mÃ¡quina local, siga os passos abaixo. Certifique-se de que vocÃª tem o Python instalado (versÃ£o 3.8+ Ã© recomendada).

### 1. Clonar o RepositÃ³rio

Primeiro, clone este repositÃ³rio para sua mÃ¡quina local usando Git:

```bash
git clone https://github.com/usuario/BOSS_IA_Recrutamento.git
cd seu-repositorio
```

Substitua `https://github.com/seu-usuario/seu-repositorio.git` pelo URL real do seu repositÃ³rio no GitHub.

### 2. Configurar o Ambiente Virtual

Ã‰ altamente recomendÃ¡vel usar um ambiente virtual para gerenciar as dependÃªncias do projeto. Isso evita conflitos com outras instalaÃ§Ãµes Python em sua mÃ¡quina.

```bash
python -m venv venv
```

### 3. Ativar o Ambiente Virtual

*   **No Windows:**
    ```bash
    .\venv\Scripts\activate
    ```

*   **No macOS/Linux:**
    ```bash
    source venv/bin/activate
    ```

### 4. Instalar as DependÃªncias

Com o ambiente virtual ativado, instale todas as bibliotecas necessÃ¡rias listadas no `requirements.txt`:

```bash
pip install -r requirements.txt
```

### 5. Configurar VariÃ¡veis de Ambiente (AWS S3)

O aplicativo Streamlit lÃª os dados processados e com features do Amazon S3. VocÃª precisarÃ¡ configurar suas credenciais AWS e o nome do bucket. Crie um arquivo `.env` na raiz do projeto com o seguinte conteÃºdo:

```
AWS_ACCESS_KEY_ID=SUA_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=SUA_SECRET_ACCESS_KEY
AWS_REGION_NAME=sua-regiao-aws # Ex: us-east-1
AWS_BUCKET_NAME=seu-nome-do-bucket-s3
```

Substitua os valores pelos seus prÃ³prios. **Nunca compartilhe este arquivo `.env` publicamente (ele jÃ¡ estÃ¡ no `.gitignore` para sua seguranÃ§a).**

### 6. Treinar o Modelo (Primeira Vez ou Retreinamento)

Antes de rodar o aplicativo, vocÃª precisa garantir que o modelo foi treinado e que os arquivos `model.pkl` e `model_columns.pkl` (e os CSVs processados) existem. Se vocÃª ainda nÃ£o treinou o modelo ou se deseja retreinÃ¡-lo com dados atualizados, execute o script `main.py`:

```bash
python -m src.main
```

Este script irÃ¡:
*   PrÃ©-processar os dados brutos (JSONs).
*   Realizar a engenharia de features.
*   Treinar o modelo de Machine Learning.
*   Avaliar o desempenho do modelo.
*   Salvar o modelo treinado e as colunas utilizadas.

**Importante:** Este passo tambÃ©m garante que os CSVs `preprocessed_data.csv` e `feature_engineered_data.csv` sejam gerados e, no seu caso, enviados para o S3, de onde o Streamlit os lerÃ¡.

### 7. Rodar o Aplicativo Streamlit

ApÃ³s o treinamento do modelo e a configuraÃ§Ã£o das variÃ¡veis de ambiente, vocÃª pode iniciar o aplicativo Streamlit:

```bash
streamlit run src/app/app.py
```

O Streamlit abrirÃ¡ automaticamente o aplicativo em seu navegador padrÃ£o (geralmente em `http://localhost:8501`). Agora vocÃª pode interagir com o sistema de matching, selecionar vagas e visualizar os candidatos recomendados.



## ğŸ”„ Como Treinar o Modelo Novamente

O processo de retreinamento do modelo Ã© integrado ao script principal do projeto (`src/main.py`). Sempre que vocÃª tiver novos dados ou desejar atualizar o modelo com as Ãºltimas informaÃ§Ãµes, basta executar este script.

### Passos para Retreinar o Modelo:

1.  **Garanta que seus dados brutos estejam atualizados:** Certifique-se de que os arquivos JSON (`applicants.json`, `prospects.json`, `vagas.json`) na pasta `src/data/raw/` contenham as informaÃ§Ãµes mais recentes que vocÃª deseja usar para o treinamento.

2.  **Execute o script principal:** Com seu ambiente virtual ativado (conforme os passos de instalaÃ§Ã£o acima), navegue atÃ© a raiz do projeto e execute:

    ```bash
    python -m src.main
    ```

    Este comando irÃ¡:
    *   Carregar os dados brutos atualizados.
    *   Executar todo o pipeline de prÃ©-processamento e engenharia de features.
    *   Treinar um novo modelo de Machine Learning do zero, utilizando os dados mais recentes.
    *   Avaliar o desempenho do novo modelo.
    *   **Salvar o novo modelo treinado** (`model.pkl`) e as colunas utilizadas (`model_columns.pkl`) na pasta `src/models/`, sobrescrevendo as versÃµes anteriores.
    *   Os CSVs processados e com features (`preprocessed_data.csv`, `feature_engineered_data.csv`) tambÃ©m serÃ£o atualizados e, no seu caso, enviados para o S3.

ApÃ³s a conclusÃ£o bem-sucedida do `src/main.py`, seu aplicativo Streamlit (quando reiniciado ou acessado) automaticamente carregarÃ¡ o modelo recÃ©m-treinado e os dados atualizados, refletindo as melhorias ou as novas informaÃ§Ãµes incorporadas.

---
